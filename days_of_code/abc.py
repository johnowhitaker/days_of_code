# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/14_Web_Scraping_ABC.ipynb (unless otherwise specified).

__all__ = ['parse_tune', 'get_tune', 'set_up_threads']

# Cell
def parse_tune(tune_content):
    t = {
        'Title': tune_content.text.split('\nT: ')[1].split('\n')[0],
        'Type':tune_content.text.split('\nR: ')[1].split('\n')[0],
        'Meter':tune_content.text.split('\nM: ')[1].split('\n')[0],
        'Length':tune_content.text.split('\nL: ')[1].split('\n')[0],
        'Key':tune_content.text.split('\nK: ')[1].split('\n')[0],
        'Notes':''.join(tune_content.text.split('\nK: ')[1].split('\n')[1:],)
    }
    return t

# Cell
def get_tune(tune_url, savefile='data/all_tunes.csv'):
    tune_page = requests.get('https://thesession.org'+tune_url)
    tune_soup = BeautifulSoup(tune_page.text, 'html5lib')
    tune_content = tune_soup.find('div', {"class": "notes"}) # Just the first arrangement
    tune = parse_tune(tune_content)
    with csv_writer_lock:
        with open(savefile, mode="a") as f1:
            review_writer = csv.writer(f1, delimiter=",")
            review_writer.writerow(tune.values())
    return tune

def set_up_threads(urls):
    with ThreadPoolExecutor(max_workers=10) as executor:
        return executor.map(get_tune,
                            urls,
                            timeout = 60)